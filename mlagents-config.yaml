# 1260 steps in a 105 second game. All values are based off of this
# train every 10 games, with opponent every 20 games

engine_settings:
  time_scale: 60
  capture_frame_rate: 60
  target_frame_rate: -1
  quality_level: 1
checkpoint_settings:
  run_id: training_run
  initialize_from: null
  load_model: false
  resume: false
  force: false
  train_model: true
  inference: false
torch_settings:
  device: cuda

behaviors:
  15robot:
    trainer_type: ppo

    hyperparameters:
      # Hyperparameters common to PPO and SAC
      batch_size: 64000
      buffer_size: 320000
      learning_rate: 3.0e-4
      learning_rate_schedule: linear

      # PPO-specific hyperparameters
      # Replaces the "PPO-specific hyperparameters" section above
      beta: 5.0e-3
      beta_schedule: constant
      epsilon: 0.2
      epsilon_schedule: linear
      lambd: 0.95
      num_epoch: 3

    # Configuration of the neural network (common to PPO/SAC)
    network_settings:
      vis_encode_type: simple
      normalize: false
      hidden_units: 512
      num_layers: 4
      # memory
      memory:
        sequence_length: 64
        memory_size: 256

    # Trainer configurations common to all trainers
    max_steps: 5.0e5
    time_horizon: 64
    summary_freq: 10000
    keep_checkpoints: 5
    checkpoint_interval: 50000
    threaded: false
    init_path: null

    
    # behavior cloning
    #behavioral_cloning:
    #  demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
    #  strength: 0.5
    #  steps: 150000
    #  batch_size: 512
    #  num_epoch: 3
    #  samples_per_update: 0

    reward_signals:
      # environment reward (default)
      extrinsic:
        strength: 1.0
        gamma: 0.99

      # curiosity module
      curiosity:
        strength: 0.02
        gamma: 0.99
        encoding_size: 256
        learning_rate: 3.0e-4

      # GAIL
      #gail:
      #  strength: 0.01
      #  gamma: 0.99
      #  encoding_size: 128
      #  #demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
      #  learning_rate: 3.0e-4
      #  use_actions: false
      #  use_vail: false

    # self-play
    self_play:
      window: 10
      play_against_latest_model_ratio: 0.5
      save_steps: 12600
      swap_steps: 25200
      team_change: 25200

###
#behaviors:
#  15robot:
#    trainer_type: ppo
#    hyperparameters:
#      batch_size: 1024
#      buffer_size: 12600
#      learning_rate: 1.0e-4
#      learning_rate_schedule: constant
#
#      num_epoch: 2
#
#    network_settings:
#      vis_encode_type: simple
#      normalize: true
#      hidden_units: 512
#      num_layers: 4
#
#    summary_freq: 12600
#    max_steps: 2000000
#
#    self_play:
#      window: 1
#      play_against_latest_model_ratio: 1.0
#      save_steps: 12600
#      swap_steps: 25200
#      team_change: 25200
